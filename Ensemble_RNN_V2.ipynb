{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logmel_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(logmel_rnn, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h_n = self.gru(x)\n",
    "        output = self.fc(h_n[-1])  # Use the hidden state from the last layer\n",
    "        return output\n",
    "    \n",
    "\n",
    "class spec_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=5):\n",
    "        super(spec_rnn, self).__init__()\n",
    "        # Initialize LSTM; note the additional argument compared to GRU\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # Linear layer to map from hidden state space to output space\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM layer\n",
    "        # h_n is the final hidden state for each element in the batch\n",
    "        # c_n is the final cell state for each element in the batch\n",
    "        _, (h_n, c_n) = self.lstm(x)\n",
    "        # We use the last hidden state to feed into the fully connected layer.\n",
    "        # The LSTM output `h_n` has dimensions [num_layers, batch, hidden_size]\n",
    "        # We take the last layer's hidden state\n",
    "        output = self.fc(h_n[-1])\n",
    "        return output\n",
    "    \n",
    "\n",
    "class chroma_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(chroma_rnn, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, input_size]\n",
    "        _, h_n = self.gru(x)\n",
    "        # h_n: [num_layers * num_directions, batch_size, hidden_size]\n",
    "        output = self.fc(h_n.squeeze(0))  # Squeeze to remove the sequence dimension\n",
    "        # output: [batch_size, output_size]\n",
    "        return output\n",
    "    \n",
    "\n",
    "class wav_rnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):\n",
    "        super(wav_rnn, self).__init__()\n",
    "        # Initialize LSTM; note the additional argument compared to GRU\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        # Linear layer to map from hidden state space to output space\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM layer\n",
    "        # h_n is the final hidden state for each element in the batch\n",
    "        # c_n is the final cell state for each element in the batch\n",
    "        _, (h_n, c_n) = self.lstm(x)\n",
    "        # We use the last hidden state to feed into the fully connected layer.\n",
    "        # The LSTM output `h_n` has dimensions [num_layers, batch, hidden_size]\n",
    "        # We take the last layer's hidden state\n",
    "        output = self.fc(h_n[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputDatset(Dataset):\n",
    "  def __init__(self, logmel_files, spec_files, chroma_files, wav_files, labels=None):\n",
    "    self.logmel_files = logmel_files\n",
    "    self.spec_files = spec_files\n",
    "    self.chroma_files = chroma_files\n",
    "    self.wav_files = wav_files\n",
    "    self.files = []\n",
    "    self.classes = os.listdir(self.logmel_files)\n",
    "    self.len = 0\n",
    "    if labels == None:\n",
    "      for cls in self.classes:\n",
    "        for file in os.listdir(os.path.join(self.logmel_files, cls)):\n",
    "          self.len += 1\n",
    "          wav_file = os.path.join(self.wav_files, cls, os.path.splitext(file)[0] + '.wav')\n",
    "          self.files.append({\n",
    "              'logmel': os.path.join(self.logmel_files, cls, file),\n",
    "              'spectrogram': os.path.join(self.spec_files, cls, file),\n",
    "              'chromagram': os.path.join(self.chroma_files, cls, file),\n",
    "              'wav': wav_file,\n",
    "              'label': cls\n",
    "          })\n",
    "    else:\n",
    "      for file in os.listdir(os.path.join(self.logmel_files)):\n",
    "        self.len += 1\n",
    "        wav_file = os.path.join(self.wav_files, os.path.splitext(file)[0] + '.wav')\n",
    "        self.files.append({\n",
    "            'logmel': os.path.join(self.logmel_files, file),\n",
    "            'spectrogram': os.path.join(self.spec_files, file),\n",
    "            'chromagram': os.path.join(self.chroma_files, file),\n",
    "            'wav': wav_file,\n",
    "            'label': labels\n",
    "        })\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.files[idx]\n",
    "    logmel_file = np.load(item['logmel'])\n",
    "    spec_file = np.load(item['spectrogram'])\n",
    "    chroma_file = np.load(item['chromagram'])\n",
    "    # wav_file = np.load(item['wav'])\n",
    "    wav_file = torchaudio.load(item['wav'])[0]\n",
    "    label = int(item['label'])\n",
    "\n",
    "    return {\n",
    "        'LOGMEL': torch.tensor(logmel_file, dtype=torch.float32),\n",
    "        'SPEC': torch.tensor(spec_file, dtype=torch.float32),\n",
    "        'CHROMA': torch.tensor(chroma_file, dtype=torch.float32),\n",
    "        'WAV': torch.tensor(wav_file, dtype=torch.float32),\n",
    "        'LABEL': label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MultiInputDatset(logmel_files='../Dataset_2_split_logmel/test', spec_files='../Dataset_2_split_spec/test', chroma_files='../Dataset_2_split_chroma/test', wav_files='../Dataset_2_split/test')\n",
    "data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmel_input_size = 128\n",
    "spec_input_size = 1025\n",
    "chroma_input_size = 12\n",
    "wav_input_size = 1\n",
    "hidden_size = 128\n",
    "output_size = 4  # Number of classes\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ModelName(Enum):\n",
    "    LOGMEL = 0\n",
    "    SPEC = 1\n",
    "    CHROMA = 2\n",
    "    WAV = 3\n",
    "\n",
    "class Genre(Enum):\n",
    "    CLASSICAL = 0\n",
    "    POP = 1\n",
    "    RNB = 2\n",
    "    ROCK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_data(data, model, model_name):\n",
    "    tensor_data = data[model_name]\n",
    "    tensor_data = tensor_data.permute(0, 2, 1)\n",
    "    tensor_data = tensor_data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_prediction = model(tensor_data)\n",
    "\n",
    "    probabilities = [F.softmax(pred, dim=0) for pred in model_prediction]\n",
    "\n",
    "    prediction_index = np.argmax(probabilities)\n",
    "    \n",
    "    prediction = 0\n",
    "\n",
    "    if prediction_index == 0:\n",
    "        prediction = 1\n",
    "    elif prediction_index == 1:\n",
    "        prediction = 4\n",
    "    elif prediction_index == 2:\n",
    "        prediction = 7\n",
    "    else: # prediction_index == 8\n",
    "        prediction = 8\n",
    "\n",
    "    return probabilities, prediction, prediction == data['LABEL'], prediction_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Mel Spectrogram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\490095775.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logmel = logmel_rnn(logmel_input_size, hidden_size, output_size)\n",
    "model_logmel.state_dict(torch.load('./logmel_rnn.pth', map_location=torch.device('cpu')))\n",
    "model_logmel.to(device)\n",
    "model_logmel.eval()\n",
    "logmel_predictions = [0, 0, 0, 0]\n",
    "logmel_correct = [0, 0, 0, 0]\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_logmel, 'LOGMEL')\n",
    "    logmel_predictions[index] = logmel_predictions[index] + 1\n",
    "    if correct:\n",
    "        print(f\"Label: {prediction}\")\n",
    "        logmel_correct[index] = logmel_correct[index] + 1\n",
    "\n",
    "del model_logmel\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrogram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28092\\2197166553.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spectrogram = spec_rnn(spec_input_size, hidden_size, output_size)\n",
    "model_spectrogram.state_dict(torch.load('./spec_rnn.pth', map_location=torch.device('cpu')))\n",
    "model_spectrogram.to(device)\n",
    "model_spectrogram.eval()\n",
    "spec_predictions = [0, 0, 0, 0]\n",
    "spec_correct = [0, 0, 0, 0]\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_spectrogram, 'SPEC')\n",
    "    spec_predictions[index] = spec_predictions[index] + 1\n",
    "    if correct:\n",
    "        print(f\"Label: {prediction}\")\n",
    "        spec_correct[index] = spec_correct[index] + 1\n",
    "\n",
    "del model_spectrogram\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromagram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28092\\2197166553.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 1\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 7\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n",
      "Label: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_chromagram = chroma_rnn(chroma_input_size, hidden_size, output_size)\n",
    "model_chromagram.state_dict(torch.load('./chroma_rnn.pth', map_location=torch.device('cpu')))\n",
    "model_chromagram.to(device)\n",
    "model_chromagram.eval()\n",
    "chroma_predictions = [0, 0, 0, 0]\n",
    "chroma_correct = [0, 0, 0, 0]\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_chromagram, 'CHROMA')\n",
    "    chroma_predictions[index] = chroma_predictions[index] + 1\n",
    "    if correct:\n",
    "        print(f\"Label: {prediction}\")\n",
    "        chroma_correct[index] = chroma_correct[index] + 1\n",
    "\n",
    "del model_chromagram\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveForm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28092\\2197166553.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n",
      "Label: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wav = wav_rnn(wav_input_size, hidden_size, output_size)\n",
    "model_wav.state_dict(torch.load('./wav_rnn.pth', map_location=torch.device('cpu')))\n",
    "model_wav.to(device)\n",
    "model_wav.eval()\n",
    "wav_predictions = [0, 0, 0, 0]\n",
    "wav_correct = [0, 0, 0, 0]\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_wav, 'WAV')\n",
    "    wav_predictions[index] = wav_predictions[index] + 1\n",
    "    if correct:\n",
    "        print(f\"Label: {prediction}\")\n",
    "        wav_correct[index] = wav_correct[index] + 1\n",
    "\n",
    "del model_wav\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logmel_correct: [0, 0, 2, 204]\n",
      "spec_correct: [0, 241, 0, 0]\n",
      "chroma_correct: [20, 0, 22, 97]\n",
      "wav_correct: [0, 241, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"logmel_correct: {logmel_correct}\")\n",
    "print(f\"spec_correct: {spec_correct}\")\n",
    "print(f\"chroma_correct: {chroma_correct}\")\n",
    "print(f\"wav_correct: {wav_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logmel_predictions: [0, 0, 16, 636]\n",
      "spec_predictions: [0, 652, 0, 0]\n",
      "chroma_predictions: [247, 0, 121, 284]\n",
      "wav_predictions: [0, 652, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"logmel_predictions: {logmel_predictions}\")\n",
    "print(f\"spec_predictions: {spec_predictions}\")\n",
    "print(f\"chroma_predictions: {chroma_predictions}\")\n",
    "print(f\"wav_predictions: {wav_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct classifications for each model by genre\n",
    "logmel_correct = [0, 0, 2, 204]\n",
    "spec_correct = [0, 241, 0, 0]\n",
    "chroma_correct = [20, 0, 22, 97]\n",
    "wav_correct = [0, 241, 0, 0]\n",
    "\n",
    "# Combining all models' correct classifications by genre\n",
    "correct_by_genre = list(zip(logmel_correct, spec_correct, chroma_correct, wav_correct))\n",
    "\n",
    "# Set the threshold for activating a model, e.g., 50% of the maximum correct predictions for each genre\n",
    "thresholds = [max(genre_correct) * 0.5 for genre_correct in correct_by_genre]\n",
    "\n",
    "# Determine which models are activated for each genre\n",
    "activated_models = [[count >= threshold for count in genre_correct] for genre_correct, threshold in zip(correct_by_genre, thresholds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_activated_models(predictions):\n",
    "    final_predictions = []\n",
    "\n",
    "    for i, genre_activations in enumerate(activated_models):\n",
    "        active_predictions = [predictions[model_index] for model_index, is_active in enumerate(genre_activations) if is_active]\n",
    "        if active_predictions:\n",
    "            # Majority vote among active models\n",
    "            predicted_genre = max(set(active_predictions), key=active_predictions.count)\n",
    "        else:\n",
    "            predicted_genre = Genre.CLASSICAL  # Default if no model is activated (choose any default as needed)\n",
    "        final_predictions.append(predicted_genre)\n",
    "\n",
    "    # Convert enum to string for readability\n",
    "    print(f\"final_predictions: {final_predictions}\")\n",
    "    # final_predictions_str = [genre.name.value for genre in final_predictions]\n",
    "    final_predictions_str = [Genre(genre_id).name for genre_id in final_predictions]\n",
    "    return max(set(final_predictions_str), key=final_predictions_str.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data):\n",
    "    predictions = [0, 0, 0, 0]\n",
    "\n",
    "    model_logmel = logmel_rnn(logmel_input_size, hidden_size, output_size)\n",
    "    model_logmel.state_dict(torch.load('./logmel_rnn.pth', map_location=torch.device('cpu')))\n",
    "    model_logmel.to(device)\n",
    "    model_logmel.eval()\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_logmel, 'LOGMEL')\n",
    "    predictions[ModelName.LOGMEL.value] = index\n",
    "    del model_logmel\n",
    "    gc.collect()\n",
    "\n",
    "    model_spectrogram = spec_rnn(spec_input_size, hidden_size, output_size)\n",
    "    model_spectrogram.state_dict(torch.load('./spec_rnn.pth', map_location=torch.device('cpu')))\n",
    "    model_spectrogram.to(device)\n",
    "    model_spectrogram.eval()\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_spectrogram, 'SPEC')\n",
    "    predictions[ModelName.SPEC.value] = index\n",
    "    del model_spectrogram\n",
    "    gc.collect()\n",
    "\n",
    "    model_chromagram = chroma_rnn(chroma_input_size, hidden_size, output_size)\n",
    "    model_chromagram.state_dict(torch.load('./chroma_rnn.pth', map_location=torch.device('cpu')))\n",
    "    model_chromagram.to(device)\n",
    "    model_chromagram.eval()\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_chromagram, 'CHROMA')\n",
    "    predictions[ModelName.CHROMA.value] = index\n",
    "    del model_chromagram\n",
    "    gc.collect()\n",
    "\n",
    "    model_wav = wav_rnn(wav_input_size, hidden_size, output_size)\n",
    "    model_wav.state_dict(torch.load('./wav_rnn.pth', map_location=torch.device('cpu')))\n",
    "    model_wav.to(device)\n",
    "    model_wav.eval()\n",
    "    probabilities, prediction, correct, index = model_predict_data(data, model_wav, 'WAV')\n",
    "    predictions[ModelName.WAV.value] = index\n",
    "    del model_wav\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    return use_activated_models(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Song Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\1026886793.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32).unsqueeze(0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [0, 1, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POP'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song = '1/6434dd126cf84c84bffa7a6ba4c8140d_snippet_0'\n",
    "\n",
    "logmel_file_path = f\"../Dataset_2_split_logmel/test/{song}.npy\"\n",
    "spec_file_path = f\"../Dataset_2_split_spec/test/{song}.npy\"\n",
    "chroma_file_path = f\"../Dataset_2_split_chroma/test/{song}.npy\"\n",
    "waveform_file_path = f\"../Dataset_2_split/test/{song}.wav\"\n",
    "\n",
    "logmel_file = np.load(logmel_file_path)\n",
    "spec_file = np.load(spec_file_path)\n",
    "chroma_file = np.load(chroma_file_path)\n",
    "wav_file = torchaudio.load(waveform_file_path)[0]\n",
    "\n",
    "data = {\n",
    "    'LOGMEL': torch.tensor(logmel_file, dtype=torch.float32).unsqueeze(0),\n",
    "    'SPEC': torch.tensor(spec_file, dtype=torch.float32).unsqueeze(0),\n",
    "    'CHROMA': torch.tensor(chroma_file, dtype=torch.float32).unsqueeze(0),\n",
    "    'WAV': torch.tensor(wav_file, dtype=torch.float32).unsqueeze(0),\n",
    "    'LABEL': '1'\n",
    "}\n",
    "\n",
    "\n",
    "get_predictions(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0.npy\n",
      "file 1.npy\n",
      "file 2.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\2212183380.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [3, 0, 3, 2]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 3, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 0, 3, 1]\n",
      "prediction ROCK\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "experiment_1_opera = MultiInputDatset(logmel_files = '../experiment_1/1/LogMelSpectogram', spec_files='../experiment_1/1/spectogram', chroma_files='../experiment_1/1/chromagram', wav_files='../experiment_1/1/audio', labels=1)\n",
    "data_loader = DataLoader(experiment_1_opera, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'OPERA':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logMel: ../experiment_1/4/LogMelSpectogram\n",
      "spec: ../experiment_1/4/spectogram\n",
      "labels 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\490095775.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [3, 3, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [2, 0, 2, 1]\n",
      "prediction RNB\n",
      "final_predictions: [1, 1, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [0, 0, 0, 3]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [3, 1, 3, 2]\n",
      "prediction ROCK\n",
      "final_predictions: [0, 0, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [0, 2, 0, 1]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 1, 1, 3]\n",
      "prediction POP\n",
      "final_predictions: [2, 2, 2, 3]\n",
      "prediction RNB\n",
      "final_predictions: [3, 3, 3, 1]\n",
      "prediction ROCK\n",
      "final_predictions: [2, 0, 2, 0]\n",
      "prediction RNB\n",
      "final_predictions: [1, 1, 1, 0]\n",
      "prediction POP\n",
      "Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "experiment_1_pop = MultiInputDatset(logmel_files = '../experiment_1/4/LogMelSpectogram', spec_files='../experiment_1/4/spectogram', chroma_files='../experiment_1/4/chromagram', wav_files='../experiment_1/4/audio', labels=4)\n",
    "data_loader = DataLoader(experiment_1_pop, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'POP':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logMel: ../experiment_1/7/LogMelSpectogram\n",
      "spec: ../experiment_1/7/spectogram\n",
      "labels 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\490095775.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [2, 2, 2, 3]\n",
      "prediction RNB\n",
      "final_predictions: [2, 1, 2, 0]\n",
      "prediction RNB\n",
      "final_predictions: [0, 1, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [3, 1, 3, 0]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 3, 3, 0]\n",
      "prediction ROCK\n",
      "final_predictions: [0, 0, 0, 0]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 3, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [0, 1, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [2, 0, 2, 1]\n",
      "prediction RNB\n",
      "final_predictions: [0, 0, 0, 1]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 3, 1, 0]\n",
      "prediction POP\n",
      "final_predictions: [3, 1, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 0, 3, 0]\n",
      "prediction ROCK\n",
      "final_predictions: [0, 0, 0, 3]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [3, 0, 3, 0]\n",
      "prediction ROCK\n",
      "final_predictions: [1, 1, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [2, 1, 2, 1]\n",
      "prediction POP\n",
      "final_predictions: [2, 0, 2, 2]\n",
      "prediction RNB\n",
      "Accuracy: 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "experiment_1_rnb = MultiInputDatset(logmel_files = '../experiment_1/7/LogMelSpectogram', spec_files='../experiment_1/7/spectogram', chroma_files='../experiment_1/7/chromagram', wav_files='../experiment_1/7/audio', labels=7)\n",
    "data_loader = DataLoader(experiment_1_rnb, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'RNB':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logMel: ../experiment_1/8/LogMelSpectogram\n",
      "spec: ../experiment_1/8/spectogram\n",
      "labels 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\490095775.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n",
      "c:\\Users\\J10ku\\miniconda3\\envs\\cs_342_pc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [3, 1, 3, 2]\n",
      "prediction ROCK\n",
      "final_predictions: [2, 0, 2, 2]\n",
      "prediction RNB\n",
      "final_predictions: [2, 0, 2, 0]\n",
      "prediction RNB\n",
      "final_predictions: [0, 0, 0, 0]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [2, 1, 2, 1]\n",
      "prediction POP\n",
      "final_predictions: [1, 1, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [3, 0, 3, 1]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 0, 3, 2]\n",
      "prediction ROCK\n",
      "final_predictions: [1, 0, 1, 1]\n",
      "prediction POP\n",
      "final_predictions: [0, 0, 0, 0]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [0, 0, 0, 1]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 0, 1, 3]\n",
      "prediction POP\n",
      "final_predictions: [1, 1, 1, 0]\n",
      "prediction POP\n",
      "final_predictions: [1, 0, 1, 3]\n",
      "prediction POP\n",
      "Accuracy: 0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "experiment_1_rock = MultiInputDatset(logmel_files = '../experiment_1/8/LogMelSpectogram', spec_files='../experiment_1/8/spectogram', chroma_files='../experiment_1/8/chromagram', wav_files='../experiment_1/8/audio', labels=8)\n",
    "data_loader = DataLoader(experiment_1_rock, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total =0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'ROCK':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acapella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [1, 1, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [2, 3, 2, 2]\n",
      "prediction RNB\n",
      "final_predictions: [3, 0, 3, 1]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 2, 3, 0]\n",
      "prediction ROCK\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "experiment_2_acapella_opera = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Acapella/1/logMelSpectogram', spec_files='../experiment_2/song_generic/Acapella/1/spectogram', chroma_files='../experiment_2/song_generic/Acapella/1/chromogram', wav_files='../experiment_2/song_generic/Acapella/1/audio', labels=1)\n",
    "data_loader = DataLoader(experiment_2_acapella_opera, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'OPERA':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [1, 2, 1, 1]\n",
      "prediction POP\n",
      "final_predictions: [2, 0, 2, 3]\n",
      "prediction RNB\n",
      "final_predictions: [3, 0, 3, 1]\n",
      "prediction ROCK\n",
      "final_predictions: [2, 0, 2, 3]\n",
      "prediction RNB\n",
      "final_predictions: [1, 0, 1, 1]\n",
      "prediction POP\n",
      "final_predictions: [3, 2, 3, 1]\n",
      "prediction ROCK\n",
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "experiment_2_acapella_pop = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Acapella/4/LogMelSpectogram', spec_files='../experiment_2/song_generic/Acapella/4/spectogram', chroma_files='../experiment_2/song_generic/Acapella/4/chromogram', wav_files='../experiment_2/song_generic/Acapella/4/audio', labels=4)\n",
    "data_loader = DataLoader(experiment_2_acapella_pop, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'POP':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [1, 0, 1, 3]\n",
      "prediction POP\n",
      "final_predictions: [2, 0, 2, 1]\n",
      "prediction RNB\n",
      "final_predictions: [0, 2, 0, 0]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 1, 1, 3]\n",
      "prediction POP\n",
      "final_predictions: [3, 0, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 0, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 1, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [0, 1, 0, 3]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 1, 1, 1]\n",
      "prediction POP\n",
      "Accuracy: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "experiment_2_acapella_rnb = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Acapella/7/LogMelSpectogram', spec_files='../experiment_2/song_generic/Acapella/7/spectogram', chroma_files='../experiment_2/song_generic/Acapella/7/chromogram', wav_files='../experiment_2/song_generic/Acapella/7/audio', labels=7)\n",
    "data_loader = DataLoader(experiment_2_acapella_rnb, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'RNB':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n",
      "c:\\Users\\J10ku\\miniconda3\\envs\\cs_342_pc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [3, 0, 3, 0]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 1, 3, 0]\n",
      "prediction ROCK\n",
      "final_predictions: [3, 1, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [2, 1, 2, 1]\n",
      "prediction POP\n",
      "final_predictions: [0, 0, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [0, 0, 0, 0]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 0, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [2, 0, 2, 3]\n",
      "prediction RNB\n",
      "final_predictions: [1, 0, 1, 2]\n",
      "prediction POP\n",
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "experiment_2_acapella_rock = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Acapella/8/rock/logMelSpectogram', spec_files='../experiment_2/song_generic/Acapella/8/spectogram', chroma_files='../experiment_2/song_generic/Acapella/8/chromagram', wav_files='../experiment_2/song_generic/Acapella/8/audio', labels=8)\n",
    "data_loader = DataLoader(experiment_2_acapella_rock, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'ROCK':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [2, 0, 2, 1]\n",
      "prediction RNB\n",
      "final_predictions: [3, 1, 3, 2]\n",
      "prediction ROCK\n",
      "final_predictions: [2, 1, 2, 3]\n",
      "prediction RNB\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "experiment_2_instrumental_opera = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Instrumental/1/logMelSpectogram', spec_files='../experiment_2/song_generic/Instrumental/1/spectogram', chroma_files='../experiment_2/song_generic/Instrumental/1/chromogram2', wav_files='../experiment_2/song_generic/Instrumental/1/audio', labels=1)\n",
    "data_loader = DataLoader(experiment_2_instrumental_opera, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'OPERA':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [0, 1, 0, 3]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [3, 1, 3, 2]\n",
      "prediction ROCK\n",
      "final_predictions: [1, 0, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [3, 2, 3, 3]\n",
      "prediction ROCK\n",
      "final_predictions: [0, 0, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [2, 3, 2, 1]\n",
      "prediction RNB\n",
      "Accuracy: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "experiment_2_instrumental_pop = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Instrumental/4/LogMelSpectogram', spec_files='../experiment_2/song_generic/Instrumental/4/spectogram', chroma_files='../experiment_2/song_generic/Instrumental/4/chromogram2', wav_files='../experiment_2/song_generic/Instrumental/4/audio', labels=4)\n",
    "data_loader = DataLoader(experiment_2_instrumental_pop, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'POP':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [0, 0, 0, 1]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [3, 0, 3, 2]\n",
      "prediction ROCK\n",
      "final_predictions: [1, 1, 1, 2]\n",
      "prediction POP\n",
      "final_predictions: [0, 0, 0, 3]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [0, 0, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [2, 0, 2, 2]\n",
      "prediction RNB\n",
      "final_predictions: [2, 1, 2, 2]\n",
      "prediction RNB\n",
      "final_predictions: [2, 0, 2, 0]\n",
      "prediction RNB\n",
      "final_predictions: [2, 2, 2, 1]\n",
      "prediction RNB\n",
      "Accuracy: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "experiment_2_instrumental_rnb = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Instrumental/7/LogMelSpectogram', spec_files='../experiment_2/song_generic/Instrumental/7/spectogram', chroma_files='../experiment_2/song_generic/Instrumental/7/chromogram2__2', wav_files='../experiment_2/song_generic/Instrumental/7/audio', labels=7)\n",
    "data_loader = DataLoader(experiment_2_instrumental_rnb, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'RNB':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\J10ku\\AppData\\Local\\Temp\\ipykernel_28976\\489830652.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'WAV': torch.tensor(wav_file, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions: [1, 0, 1, 0]\n",
      "prediction POP\n",
      "final_predictions: [0, 3, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [0, 0, 0, 1]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [2, 2, 2, 0]\n",
      "prediction RNB\n",
      "final_predictions: [0, 1, 0, 2]\n",
      "prediction CLASSICAL\n",
      "final_predictions: [1, 1, 1, 0]\n",
      "prediction POP\n",
      "final_predictions: [2, 0, 2, 3]\n",
      "prediction RNB\n",
      "final_predictions: [2, 0, 2, 0]\n",
      "prediction RNB\n",
      "final_predictions: [3, 1, 3, 2]\n",
      "prediction ROCK\n",
      "Accuracy: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "experiment_2_instrumental_rock = MultiInputDatset(logmel_files = '../experiment_2/song_generic/Instrumental/8/LogMelSpectogram', spec_files='../experiment_2/song_generic/Instrumental/8/spectogram', chroma_files='../experiment_2/song_generic/Instrumental/8/chromogram2__2', wav_files='../experiment_2/song_generic/Instrumental/8/audio', labels=8)\n",
    "data_loader = DataLoader(experiment_2_instrumental_rock, batch_size=1, shuffle=False)\n",
    "\n",
    "counter_correct = 0\n",
    "counter_total = 0\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    prediction = get_predictions(data)\n",
    "    print(f\"prediction {prediction}\")\n",
    "    counter_total = counter_total + 1\n",
    "    if prediction ==  'ROCK':\n",
    "        counter_correct = counter_correct + 1\n",
    "\n",
    "print(f\"Accuracy: {counter_correct / counter_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
